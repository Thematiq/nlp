{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:37:41.567858428Z",
     "start_time": "2023-11-21T17:37:40.986401299Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "import pickle\n",
    "import spacy\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.pl import Polish\n",
    "from math import log2\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "from functools import cached_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "   _id title                                               text metadata\n0    3        Nie mówię, że nie podoba mi się też pomysł szk...       {}\n1   31        Tak więc nic nie zapobiega fałszywym ocenom po...       {}\n2   56        Nigdy nie możesz korzystać z FSA dla indywidua...       {}\n3   59        Samsung stworzył LCD i inne technologie płaski...       {}\n4   63        Oto wymagania SEC: Federalne przepisy dotycząc...       {}",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_id</th>\n      <th>title</th>\n      <th>text</th>\n      <th>metadata</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td></td>\n      <td>Nie mówię, że nie podoba mi się też pomysł szk...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>31</td>\n      <td></td>\n      <td>Tak więc nic nie zapobiega fałszywym ocenom po...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>56</td>\n      <td></td>\n      <td>Nigdy nie możesz korzystać z FSA dla indywidua...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>59</td>\n      <td></td>\n      <td>Samsung stworzył LCD i inne technologie płaski...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>63</td>\n      <td></td>\n      <td>Oto wymagania SEC: Federalne przepisy dotycząc...</td>\n      <td>{}</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = pd.read_json('../../data/corpus.jsonl', lines=True)\n",
    "text.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:37:42.113223579Z",
     "start_time": "2023-11-21T17:37:41.568461193Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "corpus = text.set_index('_id')['text'].tolist()\n",
    "corpus = [text.lower() for text in corpus]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:37:42.282017922Z",
     "start_time": "2023-11-21T17:37:42.106060232Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Turns out later, that for some reason we have multiple spaces in some documents, causing huge problems in bigrams. Due to that, we will trim spaces in this step"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "corpus = [re.sub(r\"\\s+\", \" \", text) for text in corpus]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:37:43.866064124Z",
     "start_time": "2023-11-21T17:37:42.282919907Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Use SpaCy tokenizer API to tokenize the text from the PiQA corpus."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pl_core_news_sm\")\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:37:44.208143891Z",
     "start_time": "2023-11-21T17:37:43.867733502Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# piped_data = list(tqdm(nlp.pipe(corpus), total=len(corpus)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:37:44.213097514Z",
     "start_time": "2023-11-21T17:37:44.208260631Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# with open('piped_data.pickle', 'wb') as f:\n",
    "#     pickle.dump(piped_data, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:37:44.213254476Z",
     "start_time": "2023-11-21T17:37:44.210162146Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Compute bigram counts of downcased tokens. Given the sentence: \"The quick brown fox jumps over the lazy dog.\", the bigram counts are as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NGramData:\n",
    "    unigram_counter: dict = field(default_factory=lambda: defaultdict(lambda: 0))\n",
    "    bigram_counter: dict = field(default_factory=lambda: defaultdict(lambda: 0))\n",
    "\n",
    "    @cached_property\n",
    "    def total_unigrams(self) -> int:\n",
    "        return sum(self.unigram_counter.values())\n",
    "\n",
    "    @cached_property\n",
    "    def total_bigrams(self) -> int:\n",
    "        return sum(self.bigram_counter.values())\n",
    "    \n",
    "    def __pmi(self, bigram):\n",
    "        unigram_x, unigram_y = bigram\n",
    "        p_x = self.unigram_counter[unigram_x] / self.total_unigrams\n",
    "        p_y = self.unigram_counter[unigram_y] / self.total_unigrams\n",
    "        p_xy = self.bigram_counter[bigram] / self.total_bigrams\n",
    "    \n",
    "        ratio = p_xy / (p_x * p_y)\n",
    "    \n",
    "        return log2(ratio)\n",
    "    def pmi(self, min_occurrences=None):\n",
    "        return {\n",
    "            k: self.__pmi(k)\n",
    "            for k, v in self.bigram_counter.items()\n",
    "            if min_occurrences is None or min_occurrences < v\n",
    "        }\n",
    "\n",
    "    def count_unigram(self, unigram) -> None:\n",
    "        self.unigram_counter[unigram] += 1\n",
    "\n",
    "    def count_bigram(self, bigram) -> None:\n",
    "        self.bigram_counter[bigram] += 1\n",
    "        \n",
    "    def sanitize_bigrams(self, blacklist=r\"[^a-zA-Z\\s]\"):\n",
    "        bigrams_to_drop = [\n",
    "            key\n",
    "            for key in self.bigram_counter.keys()\n",
    "            if re.search(blacklist, \" \".join(key)) is not None\n",
    "        ]\n",
    "        for bigram in bigrams_to_drop:\n",
    "            del self.bigram_counter[bigram]\n",
    "\n",
    "def calculate_ngrams(corpus, unigram_processor=lambda x: str(x.text), unigram_validator=lambda x: True):\n",
    "    data = NGramData()\n",
    "    \n",
    "    with open('piped_data.pickle', 'rb') as f:\n",
    "        piped_data = pickle.load(f)\n",
    "    \n",
    "    for doc in tqdm(piped_data):\n",
    "        last_text = None\n",
    "        for token in doc:\n",
    "            text = unigram_processor(token)\n",
    "            \n",
    "            if not unigram_validator(token):\n",
    "                last_text = None\n",
    "                continue\n",
    "\n",
    "            data.count_unigram(text)\n",
    "            \n",
    "            if last_text is not None:\n",
    "                bigram = (last_text, text)\n",
    "                data.count_bigram(bigram)\n",
    "\n",
    "            last_text = text\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:37:44.224158576Z",
     "start_time": "2023-11-21T17:37:44.216316079Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/57638 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81ded82c4ec746c3aceb77642fb7b861"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ngram_data = calculate_ngrams(corpus) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:38:27.885377418Z",
     "start_time": "2023-11-21T17:37:44.218451619Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Discard bigrams containing characters other than letters. Make sure that you discard the invalid entries after computing the bigram counts."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "ngram_data.sanitize_bigrams()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:38:35.108813057Z",
     "start_time": "2023-11-21T17:38:27.885785294Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Use pointwise mutual information to compute the measure for all pairs of words."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "bigrams_pmi = ngram_data.pmi()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:38:35.581799637Z",
     "start_time": "2023-11-21T17:38:35.109589025Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Sort the word pairs according to that measure in the descending order and determine top 10 entries."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_best_results(pmi_values, limit=None):\n",
    "    order = sorted([(v, k) for k, v in pmi_values.items()], reverse=True)\n",
    "    if limit is not None:\n",
    "        return order[:limit]\n",
    "    return order"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:38:35.592522301Z",
     "start_time": "2023-11-21T17:38:35.583316873Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[(24.824523043569048, ('zygmunt', 'freud')),\n (24.824523043569048, ('zwaartekracht', 'pijnlijk')),\n (24.824523043569048, ('zure', 'bikotearekin')),\n (24.824523043569048, ('zur', 'tagespflege')),\n (24.824523043569048, ('zum', 'einsatz')),\n (24.824523043569048, ('zszywacz', 'pneumatyczny')),\n (24.824523043569048, ('zowel', 'anker')),\n (24.824523043569048, ('zorgeloze', 'spevener')),\n (24.824523043569048, ('zonas', 'rurales')),\n (24.824523043569048, ('zoho', 'invoice'))]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_results(bigrams_pmi, limit=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:38:36.266072082Z",
     "start_time": "2023-11-21T17:38:35.585540740Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Filter bigrams with number of occurrences lower than 5. Determine top 10 entries for the remaining dataset (>=5 occurrences)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "filtered_bigrams_pmi = ngram_data.pmi(min_occurrences=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:38:36.338693282Z",
     "start_time": "2023-11-21T17:38:36.272709065Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "[(22.23956054284789, ('seair', 'exim')),\n (22.23956054284789, ('sameer', 'thakar')),\n (22.23956054284789, ('gone', 'fishin')),\n (22.23956054284789, ('electro', 'plating')),\n (22.23956054284789, ('deming', 'electro')),\n (22.017168121511443, ('stwardnienia', 'rozsianego')),\n (22.017168121511443, ('kuala', 'lumpur')),\n (22.017168121511443, ('autot', 'ldr')),\n (21.794775700174995, ('metalami', 'szlachetnymi')),\n (21.794775700174995, ('limo', 'mia'))]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_results(filtered_bigrams_pmi, limit=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:38:36.374794123Z",
     "start_time": "2023-11-21T17:38:36.341845030Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7/8/9. Use SpaCy to lemmatize and tag the sentences in the corpus. Using the tagged corpus compute bigram statistic for the tokens containing: a. lemmatized, downcased word b. morphosyntactic category of the word (subst, fin, adj, etc.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "['to:PRED', 'być:FIN', 'testowy:ADJ', 'zdanie:ADJ']"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f\"{x.lemma_}:{x.tag_}\" \n",
    " for z in nlp.pipe(['To jest testowe zdanie'])\n",
    " for x in z]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:38:36.380416138Z",
     "start_time": "2023-11-21T17:38:36.370393984Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/57638 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df15ecbf24cb4e4dbf6a0c8b480dfa4f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ngram_data_lemma = calculate_ngrams(None, unigram_processor=lambda x: f\"{x.lemma_}:{x.tag_}\", unigram_validator=lambda x: len(str(x.tag_)) > 0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:39:23.816866780Z",
     "start_time": "2023-11-21T17:38:36.378408093Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 10. Compute the same statistics as for the non-lemmatized words (i.e. PMI) and print top-10 entries with at least 5 occurrences."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "ngram_data_lemma.sanitize_bigrams(blacklist=\"[^a-zA-Z:\\s]\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:39:29.974183912Z",
     "start_time": "2023-11-21T17:39:23.817567881Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "[(25.19152377725881, ('zygmunt:SUBST', 'freud:SUBST')),\n (25.19152377725881, ('zwei:XXX', 'investmentfonds:XXX')),\n (25.19152377725881, ('zwei:SUBST', 'anlagen:SUBST')),\n (25.19152377725881, ('zwaartekracht:SUBST', 'pijnlijk:SUBST')),\n (25.19152377725881, ('zure:SUBST', 'bikotearekin:SUBST')),\n (25.19152377725881, ('zur:SUBST', 'tagespflege:ADJ')),\n (25.19152377725881, ('zum:SUBST', 'einsatz:SUBST')),\n (25.19152377725881, ('zszywacz:SUBST', 'pneumatyczny:SUBST')),\n (25.19152377725881, ('zredukowany:ADJ', 'kompresoer:SUBST')),\n (25.19152377725881, ('zowel:XXX', 'anker:XXX'))]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_results(ngram_data_lemma.pmi(), limit=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:39:30.671867600Z",
     "start_time": "2023-11-21T17:39:29.987844660Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "[(22.606561276537654, ('seair:SUBST', 'exim:SUBST')),\n (22.606561276537654, ('sameer:SUBST', 'thakar:SUBST')),\n (22.606561276537654, ('gone:PPAS', 'fishin:SUBST')),\n (22.606561276537654, ('electro:SUBST', 'plating:SUBST')),\n (22.384168855201207, ('autot:SUBST', 'ldr:SUBST')),\n (22.021598775816496, ('agenzija:SUBST', 'sedqa:SUBST')),\n (21.79920635448005, ('mia:SUBST', 'gt:SUBST')),\n (21.776486277979966, ('dreamz:SUBST', 'infr:SUBST')),\n (21.732092158621512, ('seeking:SUBST', 'alpha:ADJ')),\n (21.732092158621512, ('document:SUBST', 'shredding:SUBST'))]"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_results(ngram_data_lemma.pmi(min_occurrences=5), limit=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:39:30.749152364Z",
     "start_time": "2023-11-21T17:39:30.682220701Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 11. Group the bigrams by morphosyntactic tag, i.e. a pair of words belongs to a given group if all pairs have the same syntactic category for the first and the second word. E.g. one group would be words with subst as the first words and adj as the second word."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NGramDataGrouped:\n",
    "    unigram_counter: dict = field(default_factory=lambda: defaultdict(lambda: 0))\n",
    "    bigram_counter: dict = field(default_factory=lambda: defaultdict(lambda: 0))\n",
    "    unigram_tag_counter: dict = field(default_factory=lambda: defaultdict(lambda: 0))\n",
    "    bigram_tag_counter: dict = field(default_factory=lambda: defaultdict(lambda: 0))\n",
    "    bigram_mapping: dict = field(default_factory=lambda: defaultdict(lambda: []))\n",
    "\n",
    "    @cached_property\n",
    "    def total_unigrams(self) -> int:\n",
    "        return sum(self.unigram_counter.values())\n",
    "\n",
    "    @cached_property\n",
    "    def total_bigrams(self) -> int:\n",
    "        return sum(self.bigram_counter.values())\n",
    "    \n",
    "    @cached_property\n",
    "    def total_tag_unigrams(self) -> int:\n",
    "        return sum(self.unigram_tag_counter.values())\n",
    "    \n",
    "    @cached_property\n",
    "    def total_tag_bigrams(self) -> int:\n",
    "        return sum(self.bigram_tag_counter.values())\n",
    "\n",
    "    def __pmi(self, bigram, bg_counter, ug_counter, bg_total, ug_total):\n",
    "        unigram_x, unigram_y = bigram\n",
    "        p_x = ug_counter[unigram_x] / ug_total\n",
    "        p_y = ug_counter[unigram_y] / ug_total\n",
    "        p_xy = bg_counter[bigram] / bg_total\n",
    "\n",
    "        ratio = p_xy / (p_x * p_y)\n",
    "\n",
    "        return log2(ratio)\n",
    "\n",
    "    def pmi(self, tags=None, min_occurrences=None):\n",
    "        bigram_source = self.bigram_counter.items() if tags is None else self.bigram_mapping[tags]\n",
    "        return {\n",
    "            k: self.__pmi(k, self.bigram_counter, self.unigram_counter, self.total_bigrams, self.total_unigrams)\n",
    "            for k in bigram_source\n",
    "            if min_occurrences is None or min_occurrences < self.bigram_counter[k]\n",
    "        }\n",
    "    \n",
    "    def pmi_tag(self, min_occurrences=None):\n",
    "        return {\n",
    "            k: self.__pmi(k, self.bigram_tag_counter, self.unigram_tag_counter, self.total_tag_bigrams, self.total_tag_unigrams)\n",
    "            for k, v in self.bigram_tag_counter.items()\n",
    "            if min_occurrences is None or min_occurrences < v\n",
    "        }\n",
    "\n",
    "    def count_unigram(self, unigram, tag) -> None:\n",
    "        self.unigram_counter[unigram] += 1\n",
    "        self.unigram_tag_counter[tag] += 1\n",
    "\n",
    "    def count_bigram(self, bigram, tag) -> None:\n",
    "        self.bigram_counter[bigram] += 1\n",
    "        self.bigram_tag_counter[tag] += 1\n",
    "        self.bigram_mapping[tag].append(bigram)\n",
    "\n",
    "    def sanitize_bigrams(self, blacklist=r\"[^a-zA-Z\\s]\"):\n",
    "        bigrams_to_drop = [\n",
    "            key\n",
    "            for key in self.bigram_counter.keys()\n",
    "            if re.search(blacklist, \" \".join(key)) is not None\n",
    "        ]\n",
    "        for bigram in bigrams_to_drop:\n",
    "            del self.bigram_counter[bigram]\n",
    "\n",
    "def calculate_grouped_ngrams():\n",
    "    data = NGramDataGrouped()\n",
    "\n",
    "    with open('piped_data.pickle', 'rb') as f:\n",
    "        piped_data = pickle.load(f)\n",
    "\n",
    "    for doc in tqdm(piped_data):\n",
    "        last_text = None\n",
    "        last_tag = None\n",
    "        for token in doc:\n",
    "            text = f\"{token.lemma_}\"\n",
    "            tag = str(token.tag_)\n",
    "\n",
    "            if len(str(tag)) == 0 or \\\n",
    "                    re.search(\"[^a-zA-Z]\", text) is not None:\n",
    "                last_text, last_tag = None, None\n",
    "                continue\n",
    "\n",
    "            data.count_unigram(text, tag)\n",
    "\n",
    "            if last_text is not None:\n",
    "                bigram = (last_text, text)\n",
    "                bigram_tag = (last_tag, tag)\n",
    "                data.count_bigram(bigram, bigram_tag)\n",
    "\n",
    "            last_text, last_tag = text, tag\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:51:29.153229392Z",
     "start_time": "2023-11-21T17:51:29.108613330Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/57638 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38e30d552a2a4b80bb51472c290c982c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = calculate_grouped_ngrams()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:52:47.704842433Z",
     "start_time": "2023-11-21T17:51:30.793177173Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 12. Print top-10 categories (sort them by total count of bigrams) and print top-5 pairs for each category."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "[(9.352670381004948, ('XXX', 'XXX')),\n (5.169646017394302, ('COMP', 'INF')),\n (3.8302810165623566, ('ADJP', 'IMPT')),\n (3.5414485128731608, ('PREP', 'ADJP')),\n (3.523885000324015, ('PREP', 'BURK')),\n (3.23781305841335, ('BURK', 'PREP')),\n (3.1574339469857073, ('IMPT', 'SIEBIE')),\n (2.644106911747437, ('QUB', 'WINIEN')),\n (2.6364093908207247, ('ADJP', 'FIN')),\n (2.339366374336545, ('PRED', '_SP'))]"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_10_tags = get_best_results(res.pmi_tag(min_occurrences=50), limit=10)\n",
    "best_10_tags"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:59:29.750611469Z",
     "start_time": "2023-11-21T17:59:29.706410445Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      " ===> TAGS ('XXX', 'XXX')\n",
      "18.704033921165102 => ('thiet', 'ke')\n",
      "18.366998933887533 => ('ding', 'ding')\n",
      "17.97468151110877 => ('pbs', 'frontline')\n",
      "17.97468151110877 => ('make', 'sure')\n",
      "17.907567315250233 => ('vous', 'devez')\n",
      "==================================================\n",
      " ===> TAGS ('COMP', 'INF')\n",
      "7.1064877186191175 => ('aby', 'zapobiec')\n",
      "4.460683257054871 => ('aby', 'uciec')\n",
      "==================================================\n",
      " ===> TAGS ('ADJP', 'IMPT')\n",
      "5.5040952601909146 => ('prosty', 'zapytaj')\n",
      "4.75754361719551 => ('prosty', 'przeczytaj')\n",
      "4.595068920238405 => ('prosty', 'mi')\n",
      "4.480248518236547 => ('prosty', 'wybierz')\n",
      "4.049919367005113 => ('prosty', 'zacznij')\n",
      "==================================================\n",
      " ===> TAGS ('PREP', 'ADJP')\n",
      "8.015027622404938 => ('po', 'prosty')\n",
      "7.888004665541665 => ('po', 'trzydziestka')\n",
      "7.343684149317855 => ('po', 'cichy')\n",
      "7.121877823185555 => ('co', 'gorsza')\n",
      "6.765952301593563 => ('od', 'dawny')\n",
      "==================================================\n",
      " ===> TAGS ('PREP', 'BURK')\n",
      "8.303042164820509 => ('po', 'trochu')\n",
      "8.015027622404938 => ('po', 'prosty')\n",
      "6.765952301593563 => ('od', 'dawny')\n",
      "6.2561925248912535 => ('z', 'dala')\n",
      "6.091553375484033 => ('na', 'jaw')\n",
      "==================================================\n",
      " ===> TAGS ('BURK', 'PREP')\n",
      "7.976338182955897 => ('dala', 'od')\n",
      "7.481192651393459 => ('zamian', 'za')\n",
      "==================================================\n",
      " ===> TAGS ('IMPT', 'SIEBIE')\n",
      "9.693788024218495 => ('zawracaj', 'siebie')\n",
      "9.497867814243238 => ('zdaj', 'siebie')\n",
      "9.382390596823301 => ('zadaj', 'siebie')\n",
      "8.512974706633447 => ('przypomnij', 'siebie')\n",
      "==================================================\n",
      " ===> TAGS ('QUB', 'WINIEN')\n",
      "3.3496245385920496 => ('nie', 'powinien')\n",
      "2.0442852550787465 => ('co', 'powinien')\n",
      "1.973293663479397 => ('przynajmniej', 'powinien')\n",
      "1.9302878330257358 => ('nie', 't')\n",
      "1.9154455565429873 => ('prawdopodobnie', 'powinien')\n",
      "==================================================\n",
      " ===> TAGS ('ADJP', 'FIN')\n",
      "4.826023355078277 => ('prosty', 'trzymanie')\n",
      "3.267227516338587 => ('prosty', 'kupujesz')\n",
      "==================================================\n",
      " ===> TAGS ('PRED', '_SP')\n",
      "5.812640381409892 => ('to', 'powiedziawsz')\n",
      "3.8522958856058933 => ('to', 'konieczny')\n",
      "3.734942113497747 => ('to', 'smutny')\n",
      "3.7221343371971267 => ('to', 'niedorzeczny')\n",
      "3.493988388630381 => ('to', 'przesada')\n"
     ]
    }
   ],
   "source": [
    "for score, tag in best_10_tags:\n",
    "    print(\"=\" * 50)\n",
    "    print(f\" ===> TAGS {tag}\")\n",
    "    \n",
    "    best_5_examples = get_best_results(res.pmi(tags=tag, min_occurrences=5), limit=5)\n",
    "    for _score, example in best_5_examples:\n",
    "        print(f\"{_score} => {example}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:59:58.643791093Z",
     "start_time": "2023-11-21T17:59:58.622712962Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 13. Create a table comparing the results for copora without and with tagging and lemmatization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "        no_lemma_no_limit           no_lemma_limit  \\\n0           zygmunt freud               seair exim   \n1  zwaartekracht pijnlijk            sameer thakar   \n2       zure bikotearekin              gone fishin   \n3         zur tagespflege          electro plating   \n4             zum einsatz           deming electro   \n5   zszywacz pneumatyczny  stwardnienia rozsianego   \n6             zowel anker             kuala lumpur   \n7      zorgeloze spevener                autot ldr   \n8           zonas rurales    metalami szlachetnymi   \n9            zoho invoice                 limo mia   \n\n                       lemma_no_limit                     lemma_limit  \n0           zygmunt:SUBST freud:SUBST          seair:SUBST exim:SUBST  \n1        zwei:XXX investmentfonds:XXX       sameer:SUBST thakar:SUBST  \n2            zwei:SUBST anlagen:SUBST          gone:PPAS fishin:SUBST  \n3  zwaartekracht:SUBST pijnlijk:SUBST     electro:SUBST plating:SUBST  \n4       zure:SUBST bikotearekin:SUBST           autot:SUBST ldr:SUBST  \n5           zur:SUBST tagespflege:ADJ      agenzija:SUBST sedqa:SUBST  \n6             zum:SUBST einsatz:SUBST              mia:SUBST gt:SUBST  \n7   zszywacz:SUBST pneumatyczny:SUBST         dreamz:SUBST infr:SUBST  \n8    zredukowany:ADJ kompresoer:SUBST         seeking:SUBST alpha:ADJ  \n9                 zowel:XXX anker:XXX  document:SUBST shredding:SUBST  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>no_lemma_no_limit</th>\n      <th>no_lemma_limit</th>\n      <th>lemma_no_limit</th>\n      <th>lemma_limit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>zygmunt freud</td>\n      <td>seair exim</td>\n      <td>zygmunt:SUBST freud:SUBST</td>\n      <td>seair:SUBST exim:SUBST</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>zwaartekracht pijnlijk</td>\n      <td>sameer thakar</td>\n      <td>zwei:XXX investmentfonds:XXX</td>\n      <td>sameer:SUBST thakar:SUBST</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>zure bikotearekin</td>\n      <td>gone fishin</td>\n      <td>zwei:SUBST anlagen:SUBST</td>\n      <td>gone:PPAS fishin:SUBST</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>zur tagespflege</td>\n      <td>electro plating</td>\n      <td>zwaartekracht:SUBST pijnlijk:SUBST</td>\n      <td>electro:SUBST plating:SUBST</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>zum einsatz</td>\n      <td>deming electro</td>\n      <td>zure:SUBST bikotearekin:SUBST</td>\n      <td>autot:SUBST ldr:SUBST</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>zszywacz pneumatyczny</td>\n      <td>stwardnienia rozsianego</td>\n      <td>zur:SUBST tagespflege:ADJ</td>\n      <td>agenzija:SUBST sedqa:SUBST</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>zowel anker</td>\n      <td>kuala lumpur</td>\n      <td>zum:SUBST einsatz:SUBST</td>\n      <td>mia:SUBST gt:SUBST</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>zorgeloze spevener</td>\n      <td>autot ldr</td>\n      <td>zszywacz:SUBST pneumatyczny:SUBST</td>\n      <td>dreamz:SUBST infr:SUBST</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>zonas rurales</td>\n      <td>metalami szlachetnymi</td>\n      <td>zredukowany:ADJ kompresoer:SUBST</td>\n      <td>seeking:SUBST alpha:ADJ</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>zoho invoice</td>\n      <td>limo mia</td>\n      <td>zowel:XXX anker:XXX</td>\n      <td>document:SUBST shredding:SUBST</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df = pd.DataFrame(\n",
    "    data={\n",
    "        'no_lemma_no_limit': [\" \".join(x[1]) for x in get_best_results(ngram_data.pmi(), limit=10)],\n",
    "        'no_lemma_limit': [\" \".join(x[1]) for x in get_best_results(ngram_data.pmi(min_occurrences=5), limit=10)],\n",
    "        'lemma_no_limit': [\" \".join(x[1]) for x in get_best_results(ngram_data_lemma.pmi(), limit=10)],\n",
    "        'lemma_limit': [\" \".join(x[1]) for x  in get_best_results(ngram_data_lemma.pmi(min_occurrences=5), limit=10)]\n",
    "    }\n",
    ")\n",
    "res_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:40:48.682705868Z",
     "start_time": "2023-11-21T17:40:47.043265489Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 14. Answer the following questions\n",
    "\n",
    "### Why do we have to filter the bigrams, rather than the token sequence?\n",
    "\n",
    "If I understand correctly (token sequence as filtering the text during processing and traversing it), we wouldn't be able to remove some bigrams without affecting the others. For example:\n",
    "\n",
    "`I like cats`\n",
    "\n",
    "If we would like to drop the sequence `I like` during the processing, we would also lose other bigram - `like cats`.\n",
    "\n",
    "### What types of expressions are discovered by the methods.\n",
    "\n",
    "In case of general **PMI** this method finds mostly proper nouns, such as people's names (Zygmunt Freud) or company name (especially with min ocurrences), such as Seair Exim or Gone Fishin."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
