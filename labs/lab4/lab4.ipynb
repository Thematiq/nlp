{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:45:25.737973Z",
     "start_time": "2023-11-20T12:45:25.730014Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.pl import Polish\n",
    "from math import log2\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "                                     id  \\\n0  f6be5fcc-d686-4549-8207-7904068693d7   \n1  ee9783b5-76a7-4beb-bbbb-9b179b11c43e   \n2  7230f9f4-06f7-4eb3-9994-762957427a96   \n3  e3304ee5-cdca-4830-b04d-a3a7cf77f6a9   \n4  b316c350-d435-4d35-a101-92ed4c9fc14a   \n\n                                                goal  \\\n0      When boiling butter, when it's ready, you can   \n1  To permanently attach metal legs to a chair, y...   \n2                       how do you indent something?   \n3                        how do you shake something?   \n4                                        Clean tires   \n\n                                                sol1  \\\n0                               Pour it onto a plate   \n1  Weld the metal together to get it to stay firm...   \n2          leave a space before starting the writing   \n3      move it up and down and side to side quickly.   \n4  Pour water, cape off caked on dirt. Use  speed...   \n\n                                                sol2  \n0                                 Pour it into a jar  \n1  Nail the metal together to get it to stay firm...  \n2                                 press the spacebar  \n3                              stir it very quickly.  \n4  Pour water, scrape off caked on dirt. Use a st...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>goal</th>\n      <th>sol1</th>\n      <th>sol2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f6be5fcc-d686-4549-8207-7904068693d7</td>\n      <td>When boiling butter, when it's ready, you can</td>\n      <td>Pour it onto a plate</td>\n      <td>Pour it into a jar</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ee9783b5-76a7-4beb-bbbb-9b179b11c43e</td>\n      <td>To permanently attach metal legs to a chair, y...</td>\n      <td>Weld the metal together to get it to stay firm...</td>\n      <td>Nail the metal together to get it to stay firm...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7230f9f4-06f7-4eb3-9994-762957427a96</td>\n      <td>how do you indent something?</td>\n      <td>leave a space before starting the writing</td>\n      <td>press the spacebar</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>e3304ee5-cdca-4830-b04d-a3a7cf77f6a9</td>\n      <td>how do you shake something?</td>\n      <td>move it up and down and side to side quickly.</td>\n      <td>stir it very quickly.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b316c350-d435-4d35-a101-92ed4c9fc14a</td>\n      <td>Clean tires</td>\n      <td>Pour water, cape off caked on dirt. Use  speed...</td>\n      <td>Pour water, scrape off caked on dirt. Use a st...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = pd.read_json('../../data/piqa/train.jsonl', lines=True)\n",
    "text.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:41:11.094320Z",
     "start_time": "2023-11-20T12:41:11.015908Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "corpus = text.melt(id_vars=['id'], value_vars=['goal', 'sol1', 'sol2'])\n",
    "corpus = corpus.set_index('id')['value'].tolist()\n",
    "corpus = [text.lower() for text in corpus]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:41:13.639381Z",
     "start_time": "2023-11-20T12:41:13.615563Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Turns out later, that for some reason we have multiple spaces in some documents, causing huge problems in bigrams. Due to that, we will trim spaces in this step"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "corpus = [re.sub(r\"\\s+\", \" \", text) for text in corpus]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:41:14.411571Z",
     "start_time": "2023-11-20T12:41:13.916188Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Use SpaCy tokenizer API to tokenize the text from the PiQA corpus."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "nlp = Polish()\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:41:14.529238Z",
     "start_time": "2023-11-20T12:41:14.519739Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Compute bigram counts of downcased tokens. Given the sentence: \"The quick brown fox jumps over the lazy dog.\", the bigram counts are as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NGramData:\n",
    "    unigram_counter: dict[str, int] = field(default_factory=lambda: defaultdict(lambda: 0))\n",
    "    bigram_counter: dict[str, int] = field(default_factory=lambda: defaultdict(lambda: 0))\n",
    "\n",
    "    @property\n",
    "    def total_unigrams(self) -> int:\n",
    "        return sum(self.unigram_counter)\n",
    "\n",
    "    @property\n",
    "    def total_bigrams(self) -> int:\n",
    "        return sum(self.bigram_counter)\n",
    "\n",
    "    def count_unigram(self, unigram) -> None:\n",
    "        self.unigram_counter[unigram] += 1\n",
    "\n",
    "    def count_bigram(self, bigram) -> None:\n",
    "        self.bigram_counter[bigram] += 1\n",
    "\n",
    "def calculate_ngrams(corpus, unigram_processor=lambda x: x.text):\n",
    "    data = NGramData()\n",
    "    for doc in tqdm(tokenizer.pipe(corpus), total=len(corpus)):\n",
    "        last_text = None\n",
    "        for token in doc:\n",
    "            text = unigram_processor(token)\n",
    "            unigram_counting[text] = unigram_counting.get(text, 0) + 1\n",
    "\n",
    "            if last_text is not None:\n",
    "                bigram = f\"{last_text} {text}\"\n",
    "                bigram_counting[bigram] = bigram_counting.get(bigram, 0) + 1\n",
    "\n",
    "            last_text = token"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:46:25.116641Z",
     "start_time": "2023-11-20T12:46:25.110512Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "unigram_counting = {}\n",
    "bigram_counting = {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:32:25.882564Z",
     "start_time": "2023-11-20T12:32:25.875464Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/48339 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "898a0d946aa340bea8ff13bf55c36ed0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for doc in tqdm(tokenizer.pipe(corpus), total=len(corpus)):\n",
    "    last_text = None\n",
    "    for token in doc:\n",
    "        text = token.text\n",
    "        unigram_counting[text] = unigram_counting.get(text, 0) + 1\n",
    "\n",
    "        if last_text is not None:\n",
    "            bigram = f\"{last_text} {text}\"\n",
    "            bigram_counting[bigram] = bigram_counting.get(bigram, 0) + 1\n",
    "\n",
    "        last_text = token"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:32:28.498874Z",
     "start_time": "2023-11-20T12:32:26.094919Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Discard bigrams containing characters other than letters. Make sure that you discard the invalid entries after computing the bigram counts."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "bigrams_to_drop = [\n",
    "    key\n",
    "    for key in bigram_counting.keys()\n",
    "    if re.search(\"[^a-zA-Z\\s]\", key) is not None\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:32:29.544672Z",
     "start_time": "2023-11-20T12:32:28.503594Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "['boiling butter,',\n 'butter, when',\n \"when it's\",\n \"it's ready,\",\n 'ready, you',\n 'a chair,',\n 'chair, you',\n 'indent something?',\n 'shake something?',\n 'taste something?']"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_to_drop[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:32:29.551356Z",
     "start_time": "2023-11-20T12:32:29.546207Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "for item in bigrams_to_drop:\n",
    "    del bigram_counting[item]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:32:29.562382Z",
     "start_time": "2023-11-20T12:32:29.560313Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Use pointwise mutual information to compute the measure for all pairs of words."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "719983"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_unigrams = sum(unigram_counting.values())\n",
    "total_unigrams"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:32:29.586870Z",
     "start_time": "2023-11-20T12:32:29.564794Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "530056"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_bigrams = sum(bigram_counting.values())\n",
    "total_bigrams"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:32:29.587311Z",
     "start_time": "2023-11-20T12:32:29.571675Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def pmi(bigram):\n",
    "    bigrams = bigram.split(' ')\n",
    "    if len(bigrams) != 2:\n",
    "        print(f\"!!! {bigram}/{bigrams}\")\n",
    "        return None\n",
    "    unigram_x, unigram_y = bigrams\n",
    "    p_x = unigram_counting[unigram_x] / total_unigrams\n",
    "    p_y = unigram_counting[unigram_y] / total_unigrams\n",
    "    p_xy = bigram_counting[bigram] / total_bigrams\n",
    "\n",
    "    ratio = p_xy / (p_x * p_y)\n",
    "\n",
    "    return log2(ratio)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:32:29.587423Z",
     "start_time": "2023-11-20T12:32:29.577561Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!   is/['', '', 'is']\n"
     ]
    }
   ],
   "source": [
    "bigrams_pmi = {\n",
    "    bigram: pmi(bigram)\n",
    "    for bigram in bigram_counting.keys()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:32:29.693618Z",
     "start_time": "2023-11-20T12:32:29.605789Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Sort the word pairs according to that measure in the descending order and determine top 10 entries."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "invalid_pmis = [k for k, v in bigrams_pmi.items() if v is None]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:32:30.528794Z",
     "start_time": "2023-11-20T12:32:30.524909Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "for k in invalid_pmis:\n",
    "    del bigrams_pmi[k]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:32:30.724790Z",
     "start_time": "2023-11-20T12:32:30.717870Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def get_best_results(pmi_values, limit=None):\n",
    "    order = sorted([(v, k) for k, v in pmi_values.items()], reverse=True)\n",
    "    if limit is not None:\n",
    "        return order[:limit]\n",
    "    return order"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:32:30.900032Z",
     "start_time": "2023-11-20T12:32:30.894789Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "[(19.899421372150968, 'zelda ii'),\n (19.899421372150968, 'wholesale distributor'),\n (19.899421372150968, 'vics vapo'),\n (19.899421372150968, 'tyrannosaurus rex'),\n (19.899421372150968, 'strontium chloride'),\n (19.899421372150968, 'storebought sheetcake'),\n (19.899421372150968, 'stinging nettle'),\n (19.899421372150968, 'stemless wineglass'),\n (19.899421372150968, 'soba noddles'),\n (19.899421372150968, 'shonda rhimes')]"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_results(bigrams_pmi, limit=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:32:31.156969Z",
     "start_time": "2023-11-20T12:32:31.150726Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Filter bigrams with number of occurrences lower than 5. Determine top 10 entries for the remaining dataset (>=5 occurrences)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "bigrams_to_filter = [k for k, v in bigram_counting.items() if v < 5]\n",
    "\n",
    "filtered_bigrams_pmi = bigrams_pmi.copy()\n",
    "\n",
    "for k in bigrams_to_filter:\n",
    "    if k not in filtered_bigrams_pmi:\n",
    "        continue\n",
    "    del filtered_bigrams_pmi[k]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:32:31.648931Z",
     "start_time": "2023-11-20T12:32:31.635508Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "[(17.092066450093366, 'chow mein'),\n (17.092066450093363, 'ping pong'),\n (17.092066450093363, 'cheeseburger copycat'),\n (16.899421372150968, 'lazy susan'),\n (16.729496370708656, 'tic tac'),\n (16.577493277263606, 'guinea pig'),\n (16.36692629132395, 'mod podge'),\n (16.092066450093366, 'girl scout'),\n (15.99253077654245, 'melon baller'),\n (15.869674028756917, 'raspberry cloud')]"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_results(filtered_bigrams_pmi, limit=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:32:32.012274Z",
     "start_time": "2023-11-20T12:32:32.009023Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7/8/9. Use SpaCy to lemmatize and tag the sentences in the corpus. Using the tagged corpus compute bigram statistic for the tokens containing: a. lemmatized, downcased word b. morphosyntactic category of the word (subst, fin, adj, etc.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "unigram_counting = {}\n",
    "bigram_counting = {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:38:22.792433Z",
     "start_time": "2023-11-20T12:38:22.788470Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/48339 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc1400cc89f640d99ca764ab9adbb83b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for doc in tqdm(tokenizer.pipe(corpus), total=len(corpus)):\n",
    "    last_text = None\n",
    "    for token in doc:\n",
    "        text = f\"{token.lemma_}:{token.tag_}\"\n",
    "        unigram_counting[text] = unigram_counting.get(text, 0) + 1\n",
    "\n",
    "        if last_text is not None:\n",
    "            bigram = f\"{last_text} {text}\"\n",
    "            bigram_counting[bigram] = bigram_counting.get(bigram, 0) + 1\n",
    "\n",
    "        last_text = token"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T12:38:24.956881Z",
     "start_time": "2023-11-20T12:38:23.047033Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
