{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " # NLP Lab 5\n",
    "\n",
    "---\n",
    "\n",
    "## Mateusz Praski"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9835b416d6f9e198"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:16:10.522166331Z",
     "start_time": "2023-11-28T18:16:10.419518675Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed, AutoModel, AutoTokenizer\n",
    "from morfeusz2 import Morfeusz\n",
    "\n",
    "import openai\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "set_seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:16:10.569544200Z",
     "start_time": "2023-11-28T18:16:10.422987878Z"
    }
   },
   "id": "78a2d7b6d3f2ac81"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "morf = Morfeusz()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:16:10.638970069Z",
     "start_time": "2023-11-28T18:16:10.427709742Z"
    }
   },
   "id": "761abf852e3aaba2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download three Polish models from the Huggingface repository. These should be regular language models, which were not fine-tuned. E.g. HerBERT and papuGaPT2 are good examples.\n",
    "\n",
    "I selected two Polish mask-filling models - BART and HerBERT, and also one multilingual model xml-roBERTa-large to check how multilingual models perform in comparison to trained one for specific language\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aac8bdacb0d76b96"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "papuga = pipeline('text-generation', model='flax-community/papuGaPT2-large')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:16:14.542857564Z",
     "start_time": "2023-11-28T18:16:10.430923146Z"
    }
   },
   "id": "4b08e66e1bba3cf3"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'generated_text': 'Idę z <mask> na spacer? Wypchaj się do tyłu z takim.\\nTak, tak, tak! Jeśli nie lubisz być agresywny, to ta pozycja przypadnie ci do gustu - może to oznaczać: bycie agresywnym, na przykład,'}]"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga(\"Idę z <mask> na spacer\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:16:18.954187887Z",
     "start_time": "2023-11-28T18:16:14.526685476Z"
    }
   },
   "id": "ec9d7f0b4a6b6578"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "herbert_base = pipeline(task=\"fill-mask\", model='allegro/herbert-base-cased')\n",
    "herbert_answer = lambda x: sorted([(x['token_str'], x['score']) for x in herbert_base(x)], key=lambda x: x[1], reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:16:19.917787123Z",
     "start_time": "2023-11-28T18:16:18.953882519Z"
    }
   },
   "id": "cf53a7d2254df37d"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "[('miasto', 0.8534282445907593),\n ('…', 0.02993311546742916),\n ('miasta', 0.024025341495871544),\n ('lotnisko', 0.02113961987197399),\n ('centrum', 0.015282508917152882)]"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(x['token_str'], x['score']) for x in herbert_base(\"Warszawa to największe <mask>\")], key=lambda x: x[1], reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:16:19.961438665Z",
     "start_time": "2023-11-28T18:16:19.918922238Z"
    }
   },
   "id": "19229f27113d0f37"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "[('miasto', 0.8534282445907593),\n ('…', 0.02993311546742916),\n ('miasta', 0.024025341495871544),\n ('lotnisko', 0.02113961987197399),\n ('centrum', 0.015282508917152882)]"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_answer(\"Warszawa to największe <mask>\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:16:19.973781145Z",
     "start_time": "2023-11-28T18:16:19.944420177Z"
    }
   },
   "id": "ab6bd2708e7845f6"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55ebf68a98994599918d76ec6da575e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57db62bb3f7f438b83d932030ff170fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad30b07e53ff4522bf2832a293f3490f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e91528df2e5c4160a52f6211c4ec6c31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta = pipeline(\"fill-mask\", \"xlm-roberta-large\")\n",
    "roberta_answer = lambda x: sorted([(x['token_str'], x['score']) for x in roberta(x)], key=lambda x: x[1], reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:00.525717310Z",
     "start_time": "2023-11-28T18:16:19.972197738Z"
    }
   },
   "id": "149fc4e85f8c3f2e"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "[('miasto', 0.9453091025352478),\n ('miasta', 0.030545899644494057),\n ('centrum', 0.012399893254041672),\n ('Miasto', 0.0023689093068242073),\n ('...', 0.0016839336603879929)]"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_answer(\"Warszawa to największe <mask>\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:00.626335847Z",
     "start_time": "2023-11-28T18:17:00.526108877Z"
    }
   },
   "id": "5a6fe6d92e8617fd"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "bart = pipeline(\"fill-mask\", \"sdadas/polish-bart-base\")\n",
    "bart_answer = lambda x: sorted([(x['token_str'], x['score']) for x in bart(x)], key=lambda x: x[1], reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:02.245413659Z",
     "start_time": "2023-11-28T18:17:00.626664207Z"
    }
   },
   "id": "9ac59abed3f4debd"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "[('miasto', 0.6284970641136169),\n ('miasta', 0.0701904147863388),\n ('centrum', 0.046382322907447815),\n ('polskie', 0.04031280800700188),\n ('...', 0.03190264850854874)]"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_answer(\"Warszawa to największe <mask>\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:02.279049929Z",
     "start_time": "2023-11-28T18:17:02.246961923Z"
    }
   },
   "id": "3de5b9f8d143c8b8"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0,\n  1,\n  ('pies', 'pies:Sm1', 'subst:sg:nom:m1', ['nazwa_pospolita'], ['pot.'])),\n (0, 1, ('pies', 'pies:Sm2', 'subst:sg:nom:m2', ['nazwa_pospolita'], [])),\n (1,\n  2,\n  ('psa', 'pies:Sm1', 'subst:sg:gen.acc:m1', ['nazwa_pospolita'], ['pot.'])),\n (1, 2, ('psa', 'pies:Sm2', 'subst:sg:gen.acc:m2', ['nazwa_pospolita'], []))]"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morf.analyse('pies psa')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:02.283329263Z",
     "start_time": "2023-11-28T18:17:02.279252368Z"
    }
   },
   "id": "a2e61aa05ced5e32"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def get_predicted_declension(word):\n",
    "    return set([z\n",
    "                    for x in morf.analyse(word)\n",
    "                    if x[2][2].split(':')[0] == 'subst'\n",
    "                    for z in x[2][2].split(':')[2].split('.')])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:02.292026364Z",
     "start_time": "2023-11-28T18:17:02.282002717Z"
    }
   },
   "id": "41404c13d329ed3f"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def get_nom_declension(word):\n",
    "    rs = [x[2][1].split(':')[0] for x in morf.analyse(word)]\n",
    "    if len(rs) != 1:\n",
    "        # print(f\"!!!! {rs}\")\n",
    "        pass\n",
    "    return rs[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:02.294010417Z",
     "start_time": "2023-11-28T18:17:02.285031887Z"
    }
   },
   "id": "4323295ecd035e5d"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "{'inst'}"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_declension('klubem')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:02.365189282Z",
     "start_time": "2023-11-28T18:17:02.286690820Z"
    }
   },
   "id": "30111fa0a8c8990f"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "'klub'"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nom_declension('klubem')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:02.365884304Z",
     "start_time": "2023-11-28T18:17:02.330084605Z"
    }
   },
   "id": "9653c5c78efc7b8a"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "polish_test = {\n",
    "    # declension -> (query, expected, nom word)\n",
    "    'mianownik': ('Warszawa to największe <mask>', 'nom', 'miasto'),\n",
    "    'dopełniacz': ('Mój kolega nie ma <mask>. Musi przez to jeździć pociągiem do pracy', 'gen', 'auto'),\n",
    "    'celownik': ('W <mask> na lepsze życie, kupiłem los na loterii', 'dat', 'nadzieja'),\n",
    "    'biernik': ('Jestem roztrzepany, więc zapomniałem kupić <mask>', 'acc', 'ser'),\n",
    "    'narzędnik': ('Idę z <mask> na spacer', 'inst', 'pies'),\n",
    "    'miejscownik': ('Gdy jest mi źle, myślę o <mask>', 'loc', 'kot'),\n",
    "    'wołacz': ('<mask>, Ojczyzno Moja!', 'voc', 'litwa')\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:02.366071523Z",
     "start_time": "2023-11-28T18:17:02.330328029Z"
    }
   },
   "id": "6a8a7b67eafa5921"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    answers = {}\n",
    "    \n",
    "    for dec, (query, expected, base) in polish_test.items():\n",
    "        ans = model(query)[0][0]\n",
    "        # print(f\">>{ans}<<\")\n",
    "        actual_dec = get_predicted_declension(ans.lower())\n",
    "        actual_word = get_nom_declension(ans)\n",
    "        \n",
    "        points = 0\n",
    "        if expected in actual_dec:\n",
    "            points += 10\n",
    "        if actual_word == base:\n",
    "            points += 2\n",
    "        \n",
    "        answers[dec] = {\n",
    "            'answer': ans,\n",
    "            'dec': actual_dec,\n",
    "            'word': actual_word,\n",
    "            'points': points,\n",
    "            'text': re.sub(\"<mask>\", ans, query)\n",
    "        }\n",
    "    \n",
    "    return answers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:02.366166690Z",
     "start_time": "2023-11-28T18:17:02.330450717Z"
    }
   },
   "id": "6109f146ef18f881"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "Model   herbert\n",
      "Points  42\n",
      "mianownik => Warszawa to największe miasto\n",
      "dopełniacz => Mój kolega nie ma samochodu. Musi przez to jeździć pociągiem do pracy\n",
      "celownik => W oczekiwaniu na lepsze życie, kupiłem los na loterii\n",
      "biernik => Jestem roztrzepany, więc zapomniałem kupić .\n",
      "narzędnik => Idę z Tobą na spacer\n",
      "miejscownik => Gdy jest mi źle, myślę o sobie\n",
      "wołacz => Witaj, Ojczyzno Moja!\n",
      "============================\n",
      "Model   roberta\n",
      "Points  42\n",
      "mianownik => Warszawa to największe miasto\n",
      "dopełniacz => Mój kolega nie ma samochodu. Musi przez to jeździć pociągiem do pracy\n",
      "celownik => W drodze na lepsze życie, kupiłem los na loterii\n",
      "biernik => Jestem roztrzepany, więc zapomniałem kupić .\n",
      "narzędnik => Idę z ним na spacer\n",
      "miejscownik => Gdy jest mi źle, myślę o Bogu\n",
      "wołacz => Polska, Ojczyzno Moja!\n",
      "============================\n",
      "Model   bart\n",
      "Points  42\n",
      "mianownik => Warszawa to największe miasto\n",
      "dopełniacz => Mój kolega nie ma pracy. Musi przez to jeździć pociągiem do pracy\n",
      "celownik => W poszukiwaniu na lepsze życie, kupiłem los na loterii\n",
      "biernik => Jestem roztrzepany, więc zapomniałem kupić .\n",
      "narzędnik => Idę z nimi na spacer\n",
      "miejscownik => Gdy jest mi źle, myślę o sobie\n",
      "wołacz => O, Ojczyzno Moja!\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in {'herbert': herbert_answer, 'roberta': roberta_answer, 'bart': bart_answer}.items():\n",
    "    res = test(model)\n",
    "    print(\"============================\")\n",
    "    print(f\"Model   {model_name}\")\n",
    "    print(f\"Points  {sum([x['points'] for x in res.values()])}\")\n",
    "    for dec, rs in res.items():\n",
    "        print(f\"{dec} => {rs['text']}\")\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T19:40:57.502177076Z",
     "start_time": "2023-11-28T19:40:56.339745711Z"
    }
   },
   "id": "97c31d56d8892146"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Devise a method to test long-range relationships such as gender. E.e. you can use two verbs with masculine and feminine gender, where one of the verbs is masked. Both verbs should have the same gender, assuming the subject is the same. Define at least 3 such sentences."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e86431c0c81ae750"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "fill_sentence = lambda model, sentence: re.sub(\"<mask>\", model(sentence)[0][0], sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:03.403619106Z",
     "start_time": "2023-11-28T18:17:03.361363971Z"
    }
   },
   "id": "390057870340fb0d"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "sen_1 = \"Wczoraj miałem pojechać do rodziców, ale w natłoku rzeczy <mask> o tym\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:03.404068391Z",
     "start_time": "2023-11-28T18:17:03.403449697Z"
    }
   },
   "id": "60e83497f6dd8ce3"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "'Wczoraj miałem pojechać do rodziców, ale w natłoku rzeczy myślałem o tym'"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(herbert_answer, sen_1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:03.406837157Z",
     "start_time": "2023-11-28T18:17:03.403906196Z"
    }
   },
   "id": "43564bfa2795ef58"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "'Wczoraj miałem pojechać do rodziców, ale w natłoku rzeczy nie o tym'"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(roberta_answer, sen_1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:03.524094975Z",
     "start_time": "2023-11-28T18:17:03.404189290Z"
    }
   },
   "id": "3ddf22cc5f9fad9f"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "'Wczoraj miałem pojechać do rodziców, ale w natłoku rzeczy nie o tym'"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(bart_answer, sen_1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:03.573524522Z",
     "start_time": "2023-11-28T18:17:03.523734266Z"
    }
   },
   "id": "6c1bf2ab3437301"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "sen_2 = 'Ala uwielbiała swojego kota, do tego stopnia, że w podczas podróży <mask> do niego'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:03.573810968Z",
     "start_time": "2023-11-28T18:17:03.566794973Z"
    }
   },
   "id": "f74af6d2414e940f"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "'Ala uwielbiała swojego kota, do tego stopnia, że w podczas podróży chodziła do niego'"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(herbert_answer, sen_2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:03.624519170Z",
     "start_time": "2023-11-28T18:17:03.566971182Z"
    }
   },
   "id": "93d12b90e71f312a"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "'Ala uwielbiała swojego kota, do tego stopnia, że w podczas podróży wraca do niego'"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(roberta_answer, sen_2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:03.712293689Z",
     "start_time": "2023-11-28T18:17:03.597264660Z"
    }
   },
   "id": "22de7cdf7d95850f"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "'Ala uwielbiała swojego kota, do tego stopnia, że w podczas podróży zawsze do niego'"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(bart_answer, sen_2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:03.773359928Z",
     "start_time": "2023-11-28T18:17:03.711926682Z"
    }
   },
   "id": "4c9391c53ea34259"
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "sen_3 = \"Aleksandra była pierworoczną cheerleaderką drużyny licealnej. Kamil kapitan drużyny piłki nożnej, był w niej zauroczony od pierwszego wejrzenia. Niestety dla szans Kamila, nic do niego nie <mask>\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T19:45:02.875993513Z",
     "start_time": "2023-11-28T19:45:02.832146808Z"
    }
   },
   "id": "4dd7d064a24087de"
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "data": {
      "text/plain": "'Aleksandra była pierworoczną cheerleaderką drużyny licealnej. Kamil kapitan drużyny piłki nożnej, był w niej zauroczony od pierwszego wejrzenia. Niestety dla szans Kamila, nic do niego nie …'"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(herbert_answer, sen_3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T19:45:03.266386705Z",
     "start_time": "2023-11-28T19:45:03.183887948Z"
    }
   },
   "id": "b3707ea7d768b609"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "'Aleksandra była pierworoczną cheerleaderką drużyny licealnej. Kamil kapitan drużyny piłki nożnej, był w niej zauroczony od pierwszego wejrzenia. Niestety dla szans Kamila, nic do niego nie ...'"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(roberta_answer, sen_3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T19:45:04.460760433Z",
     "start_time": "2023-11-28T19:45:04.156565983Z"
    }
   },
   "id": "b8910b5d556bce38"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "data": {
      "text/plain": "'Aleksandra była pierworoczną cheerleaderką drużyny licealnej. Kamil kapitan drużyny piłki nożnej, był w niej zauroczony od pierwszego wejrzenia. Niestety dla szans Kamila, nic do niego nie dociera'"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(bart_answer, sen_3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T19:45:05.111109040Z",
     "start_time": "2023-11-28T19:45:05.018349954Z"
    }
   },
   "id": "191fca6c18debc6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Check if the model captures real-world knolwedge. For instance a sentence \"[MASK] wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\" checks if the model \"knows\" the description of water. Define at least 3 such sentences."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25665d7eaf05cef"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "q_1 = \"<mask> wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:04.105145976Z",
     "start_time": "2023-11-28T18:17:04.060435732Z"
    }
   },
   "id": "fe82406bdc5c2d1f"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "'Woda wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza'"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(herbert_answer, q_1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:04.106178032Z",
     "start_time": "2023-11-28T18:17:04.103544809Z"
    }
   },
   "id": "a8592e3b2df40ed0"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "'Olej wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza'"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(roberta_answer, q_1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:04.223960800Z",
     "start_time": "2023-11-28T18:17:04.103808258Z"
    }
   },
   "id": "c6bd8f5050a3b052"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "'W wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza'"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(bart_answer, q_1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:04.288174988Z",
     "start_time": "2023-11-28T18:17:04.223618865Z"
    }
   },
   "id": "1aced33863c5d29c"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "q_2 = \"<mask> 13 grudnia 1981 roku wprowadził w Polsce stan wojenny\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:04.288433719Z",
     "start_time": "2023-11-28T18:17:04.258925775Z"
    }
   },
   "id": "83d12639a5b08cf5"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "'Sejm 13 grudnia 1981 roku wprowadził w Polsce stan wojenny'"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(herbert_answer, q_2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:04.338708596Z",
     "start_time": "2023-11-28T18:17:04.261460324Z"
    }
   },
   "id": "64c49e23432386ad"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "'- 13 grudnia 1981 roku wprowadził w Polsce stan wojenny'"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(roberta_answer, q_2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:04.404824785Z",
     "start_time": "2023-11-28T18:17:04.303414294Z"
    }
   },
   "id": "d396fe0cef2ef050"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "'13 13 grudnia 1981 roku wprowadził w Polsce stan wojenny'"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(bart_answer, q_2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:04.463555855Z",
     "start_time": "2023-11-28T18:17:04.405239868Z"
    }
   },
   "id": "ac9b24b69f394471"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "q_3 = \"Iloczyn dwóch liczb ujemnych jest liczbą <mask>\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:04.463877461Z",
     "start_time": "2023-11-28T18:17:04.446816772Z"
    }
   },
   "id": "f7c945ac780309f1"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "'Iloczyn dwóch liczb ujemnych jest liczbą :'"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(herbert_answer, q_3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:04.477138925Z",
     "start_time": "2023-11-28T18:17:04.447171298Z"
    }
   },
   "id": "7f2c71e54bde7fe2"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "'Iloczyn dwóch liczb ujemnych jest liczbą :'"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(roberta_answer, q_3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:04.586388700Z",
     "start_time": "2023-11-28T18:17:04.469230363Z"
    }
   },
   "id": "a0f70fecff4fa2b1"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "'Iloczyn dwóch liczb ujemnych jest liczbą dodat'"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_sentence(bart_answer, q_3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:04.630874899Z",
     "start_time": "2023-11-28T18:17:04.585870401Z"
    }
   },
   "id": "31c9c334a0029a7d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Check zero-shot learning capabilites of the models. Provide at least 5 sentences with different sentiment for the following scheme: \"'Ten film to był kiler. Nie mogłem się oderwać od ekranu.' Wypowiedź ta ma jest zdecydowanie [MASK]\" Try different prompts, to see if they make any difference."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be8a7a5b9feef005"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "predict_sentiment = lambda model, prompt, query: model(re.sub(\"<X>\", query, prompt))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:04.631099096Z",
     "start_time": "2023-11-28T18:17:04.626632179Z"
    }
   },
   "id": "6129c00771549fd3"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "prompt_1 = \"`<X>`. Wypowiedź ta miała zdecydowanie charakter <mask>\"\n",
    "prompt_2 = \"Recenzje filmowe mogą być pozytywne, negatywne, bądź neutralne. Dla przykładu ta wypowiedź `<X>` jest <mask>\"\n",
    "prompt_3 = \"Moim zdaniem wypowiedź `<X>` wypowiada się <mask> na temat filmu\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:04.631211176Z",
     "start_time": "2023-11-28T18:17:04.626758637Z"
    }
   },
   "id": "fd5e46ef4680414f"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "review_0 = \"Ten film to był kiler. Nie mogłem się oderwać od ekranu.\"\n",
    "review_1 = \"Wojna żeńsko-męska to film, który wgl nie powinien powstać. Pani Samson (o dziwo - psycholożka, pisarka, dziennikarka, zupełnie jak główna bohaterka filmu, Barbara Patrycka) stworzyła beznadziejny scenariusz, a pan Palkowski przy dobrym Rezerwacie stracił w oczach widzów. \"\n",
    "review_2 = \"Hipnotyzujący i olśniewający wizualnie. Reymont odczytany na nowo.\"\n",
    "review_3 = \"Główny bohater jest wręcz obleśnie lubialny. Niepotrzebna próba wrzucenia lekcji o walce z systemem, ale wspaniale ogląda się zachowania postaci, charaktery.\"\n",
    "review_4 = \"To było tak złe, że mam ochotę podnieść oceny wszystkim filmom, które oceniłam na 1. Arcydzieło bezguścia. Ostrzegam: nie oglądajcie tego na trzeźwo!\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:19:58.550042427Z",
     "start_time": "2023-11-28T18:19:58.509977310Z"
    }
   },
   "id": "a09d8c16967b9fa"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "'Moim zdaniem wypowiedź `Wojna żeńsko-męska to film, który wgl nie powinien powstać. Pani Samson (o dziwo - psycholożka, pisarka, dziennikarka, zupełnie jak główna bohaterka filmu, Barbara Patrycka) stworzyła beznadziejny scenariusz, a pan Palkowski przy dobrym Rezerwacie stracił w oczach widzów. ` wypowiada się <mask> na temat filmu'"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"<X>\", review_1, prompt_3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:17:04.631477505Z",
     "start_time": "2023-11-28T18:17:04.626887057Z"
    }
   },
   "id": "2dd26112a78081ef"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= pozytywna ==========\n",
      "      bart - prompt 1 - review 0 -> ironi\n",
      "      bart - prompt 2 - review 0 -> bardzo\n",
      "      bart - prompt 3 - review 0 -> w\n",
      "   herbert - prompt 1 - review 0 -> .\n",
      "   herbert - prompt 2 - review 0 -> …\n",
      "   herbert - prompt 3 - review 0 -> negatywnie\n",
      "   roberta - prompt 1 - review 0 -> ...\n",
      "   roberta - prompt 2 - review 0 -> ...\n",
      "   roberta - prompt 3 - review 0 -> idealnie\n",
      "======= negatwyna ==========\n",
      "      bart - prompt 1 - review 1 -> ironi\n",
      "      bart - prompt 2 - review 1 -> to\n",
      "      bart - prompt 3 - review 1 -> w\n",
      "   herbert - prompt 1 - review 1 -> .\n",
      "   herbert - prompt 2 - review 1 -> .\n",
      "   herbert - prompt 3 - review 1 -> krytycznie\n",
      "   roberta - prompt 1 - review 1 -> .\n",
      "   roberta - prompt 2 - review 1 -> ...\n",
      "   roberta - prompt 3 - review 1 -> źle\n",
      "======= pozytywna ==========\n",
      "      bart - prompt 1 - review 2 -> proro\n",
      "      bart - prompt 2 - review 2 -> bardzo\n",
      "      bart - prompt 3 - review 2 -> w\n",
      "   herbert - prompt 1 - review 2 -> .\n",
      "   herbert - prompt 2 - review 2 -> …\n",
      "   herbert - prompt 3 - review 2 -> negatywnie\n",
      "   roberta - prompt 1 - review 2 -> ...\n",
      "   roberta - prompt 2 - review 2 -> :\n",
      "   roberta - prompt 3 - review 2 -> najlepiej\n",
      "======= pozytywna ==========\n",
      "      bart - prompt 1 - review 3 -> humo\n",
      "      bart - prompt 2 - review 3 -> bardzo\n",
      "      bart - prompt 3 - review 3 -> w\n",
      "   herbert - prompt 1 - review 3 -> .\n",
      "   herbert - prompt 2 - review 3 -> pozytywna\n",
      "   herbert - prompt 3 - review 3 -> krytycznie\n",
      "   roberta - prompt 1 - review 3 -> .\n",
      "   roberta - prompt 2 - review 3 -> ...\n",
      "   roberta - prompt 3 - review 3 -> źle\n",
      "======= negatywna ==========\n",
      "      bart - prompt 1 - review 4 -> humo\n",
      "      bart - prompt 2 - review 4 -> bardzo\n",
      "      bart - prompt 3 - review 4 -> w\n",
      "   herbert - prompt 1 - review 4 -> .\n",
      "   herbert - prompt 2 - review 4 -> …\n",
      "   herbert - prompt 3 - review 4 -> negatywnie\n",
      "   roberta - prompt 1 - review 4 -> ...\n",
      "   roberta - prompt 2 - review 4 -> ...\n",
      "   roberta - prompt 3 - review 4 -> najlepiej\n"
     ]
    }
   ],
   "source": [
    "for idr, (review, rs) in enumerate(zip([review_0, review_1, review_2, review_3, review_4], ['pozytywna', 'negatwyna', 'pozytywna', 'pozytywna', 'negatywna'])):\n",
    "    print(f\"======= {rs} ==========\")\n",
    "    for model_name, model in {'bart': bart_answer, 'herbert': herbert_answer, 'roberta': roberta_answer}.items():\n",
    "        for idp, prompt in enumerate([prompt_1, prompt_2, prompt_3]):\n",
    "            res = predict_sentiment(model, prompt, review)[0][0]\n",
    "            print(f\"{model_name:>10} - prompt {idp + 1} - review {idr} -> {res}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T18:20:17.464624016Z",
     "start_time": "2023-11-28T18:20:13.422074056Z"
    }
   },
   "id": "730d19124b8024bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Answer the following questions (2 points):\n",
    "\n",
    "## Which of the models produced the best results?\n",
    "\n",
    "I think that the best model by far is herBERT. Firstly, it hadn't had any strange response (in comparison to the \"partially spelled\" words by Bart, and Cyrillic from roBERTa. Also, his responses had the most sense.\n",
    "\n",
    "## Was any of the models able to capture Polish grammar?\n",
    "\n",
    "Both roBERTa-large and herBERT filled the answers with proper grammar and flexion in almost all the cases.\n",
    "\n",
    "BART very often generated weird responses (e.g. `humo`, `'W wrze w...` or `jest liczbą dodat`)\n",
    "\n",
    "## Was any of the models able to capture long-distant relationships between the words?\n",
    "\n",
    "With the 4th task, herBERT was the best, answering all the questions correctly (although he chose some words that didn't need proper verb gender)\n",
    "\n",
    "BART and roBERTa performed very similar.\n",
    "\n",
    "## Was any of the models able to capture world knowledge?\n",
    "\n",
    "Again the closest one was herBERT, where he answered correctly water, and tried his best with martial law.\n",
    "\n",
    "On the other hand, BART as the only one, answered what's the product of tho negative values (although he said `dodat` not `dodatnia`)\n",
    "\n",
    "## Was any of the models good at doing zero-shot classification?\n",
    "\n",
    "The best one in my opinion was herBERT with 3rd prompt `Moim zdaniem wypowiedź `<X>` wypowiada się <mask> na temat filmu`\n",
    "\n",
    "On the other hand roBERTa was classifying the reviews the same way as student which is not prepared for the test (like `this text reviews the movie perfectly`)\n",
    "\n",
    "Most of the time, models just put out single dot as the answer (I'm not sure what does it mean)\n",
    "\n",
    "## What are the most striking errors made by the models?\n",
    "\n",
    "- roBERTa answering one question using Russian instead of Polish\n",
    "- Plenty of random words generated by BART\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "132584336e23fe41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1f4f6384c55370e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
